{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import Data\n",
    "import utils\n",
    "import Predictors, Road_Graph\n",
    "imp.reload(Data)\n",
    "imp.reload(utils)\n",
    "imp.reload(Predictors)\n",
    "imp.reload(Road_Graph)\n",
    "\n",
    "from Road_Graph import *\n",
    "from Predictors import *\n",
    "from Data import *\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch import nn, autograd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we focus on a simple road map (rep as a graph below), and generate random path on it.\n",
    "<img src=\"../img/naive_road.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the road graph as above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "graph_dict = graph_dict.fromkeys(range(24))\n",
    "edges = [(i, i+1) for i in range(5)] \\\n",
    "        + [(i+6, i+7) for i in range(5)]\\\n",
    "                        +[(i+12, i+13) for i in range(5)]\\\n",
    "                     + [(i+18, i+19) for i in range(5)]\\\n",
    "                     + [(0,6),(6,12), (12,18)]\\\n",
    "                     + [(pair[0]+1, pair[1]+1) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+2, pair[1]+2) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+3, pair[1]+3) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+4, pair[1]+4) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+5, pair[1]+5) for pair in [(0,6),(6,12), (12,18)]]\n",
    "for i,j in edges:\n",
    "    if not graph_dict[i]:\n",
    "        graph_dict[i] = [j]\n",
    "    else:\n",
    "        graph_dict[i].append(j)\n",
    "    if not graph_dict[j]:\n",
    "        graph_dict[j] = [i]\n",
    "    else:\n",
    "        graph_dict[j].append(i)\n",
    "\n",
    "#display(graph_dict)\n",
    "graph = Road_Graph(graph_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = random_walk_data(graph, 10000,go_back = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_pd = Markov_Predictor(graph)\n",
    "mk_pd.train(data)\n",
    "nxt,prob = mk_pd.predict([0,1,2])\n",
    "print(nxt, prob)\n",
    "mk_pd.eval(random_walk_data(graph, 1000,go_back = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.neighbors([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = random_walk_data(graph, 10000, go_back = True)\n",
    "\n",
    "D_in, D_hidden, D_out = 24, 100, 24\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    "    torch.nn.Softmax(dim = 1),\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 0.002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "L = 0\n",
    "n = 0\n",
    "pts = {'x':[], 'y':[]}\n",
    "for t in range(1,20001):\n",
    "    path = data[t % len(data)]\n",
    "    x,y = path_to_training_pair(path, graph)\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    L += sum(loss.data.numpy())\n",
    "    n += len(loss.data.numpy())\n",
    "    if t%1000 == 0:\n",
    "        pts['x'].append(t)\n",
    "        pts['y'].append(L/n)\n",
    "        print(t, L/n)\n",
    "        L = 0\n",
    "        n = 0\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pts['x'],pts['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = training_set(data[0], graph)\n",
    "print(inv_one_hot(x.data.numpy()))\n",
    "print(graph.neighbors(11))\n",
    "print(model(x)[1][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_loss(path):\n",
    "    def lable(x_raw, y):\n",
    "        # x,y are road level\n",
    "        nb = graph.neighbors(x_raw)\n",
    "        z = y.data[0]\n",
    "        return to_Variable([np.where(np.array(nb) == z)[0][0]])\n",
    "    def pred(x_raw, y_):\n",
    "        nb = graph.neighbors(x_raw)\n",
    "        return F.softmax(torch.cat([y_[n] for n in nb]), dim=0)\n",
    "    path = data[1]\n",
    "    X_raw = path[:-1]\n",
    "    X,Y = path_to_training_pair(path,graph)\n",
    "    Y_ = model(X)\n",
    "    loss = []\n",
    "    for i in range(len(Y)):\n",
    "        x, y, y_, x_raw = X[i], Y[i], Y_[i], X_raw[i]\n",
    "        loss.append(F.cross_entropy(pred(x_raw,y_).view(1,-1),  lable(x_raw,y))) \n",
    "    return torch.sum(torch.cat(loss))\n",
    "\n",
    "def train_step(path):\n",
    "    loss = comp_loss(path)\n",
    "    optimizer.zero_grad()                                                                                                                                                  \n",
    "    loss.backward()                                                                                                                                                        \n",
    "    optimizer.step()\n",
    "def pred(x_raw):\n",
    "    nb = graph.neighbors(x_raw)\n",
    "    y_ = model(to_Variable(one_hot(x_raw)))\n",
    "    return nb, F.softmax(torch.cat([y_[n] for n in nb]), dim=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path\n",
    "X,Y = path_to_training_pair(path,graph)\n",
    "x,y = X[0],Y[0]\n",
    "inv_one_hot(x.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw = inv_one_hot(x.data.numpy())\n",
    "y_ = model(x[0])\n",
    "nb = graph.neighbors(x_raw)\n",
    "y_ = model(to_Variable(one_hot(x_raw)))\n",
    "print(nb, F.softmax(torch.cat([y_[n] for n in nb]), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x_raw):\n",
    "    nb = graph.neighbors(x_raw)\n",
    "    y_ = model(to_Variable(one_hot(x_raw, 24)))\n",
    "    return nb, F.softmax(torch.cat([y_[n] for n in nb]), dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = random_walk_data(graph, 10000, go_back = True)\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.002 \n",
    "steps = 20000\n",
    "D_in, D_hidden, D_out = len(graph.nodes), 100, len(graph.nodes)                                                                                                                 \n",
    "\n",
    "model = torch.nn.Sequential(                                                                                                                                               \n",
    "    torch.nn.Linear(D_in, D_hidden),                                                                                                                                       \n",
    "    torch.nn.ReLU(),                                                                                                                                                      \n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    ")                                                                                                                                             \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)                                                                                                         \n",
    "num_batch = len(data) // batch_size\n",
    "\n",
    "losses = []\n",
    "for t in range(1,steps+1):\n",
    "    i = t % num_batch\n",
    "    batch_loss = Variable(torch.zeros(1), requires_grad=True)\n",
    "    #num_road = 0\n",
    "    y_pred_list = []\n",
    "    y_list = []\n",
    "    for path in data[i*batch_size:(i+1)*batch_size]:\n",
    "        train_step(path)\n",
    "    if t%1000 == 0:\n",
    "        print(t)#, batch_loss.data[0])\n",
    "    #losses.append(batch_loss.data[0])\n",
    "    if i == 0:\n",
    "        np.random.shuffle(data)\n",
    "    #optimizer.zero_grad()                                                                                                                                                  \n",
    "    #loss.backward()                                                                                                                                                        \n",
    "    #optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = training_set(data[0], graph)\n",
    "print(inv_one_hot(x.data.numpy()))\n",
    "#print(mk_pd.predict([]))\n",
    "print(graph.neighbors(5))\n",
    "print(model(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(self, data, batch_size = 50, learning_rate = 0.001, steps = 10000):                                                                                                  \n",
    "data = random_walk_data(graph, 10000, go_back = True)\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.002 \n",
    "steps = 20000\n",
    "D_in, D_hidden, D_out = len(graph.nodes), 100, len(graph.nodes)                                                                                                                 \n",
    "\n",
    "model = torch.nn.Sequential(                                                                                                                                               \n",
    "    torch.nn.Linear(D_in, D_hidden),                                                                                                                                       \n",
    "    torch.nn.ReLU(),                                                                                                                                                      \n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    "    torch.nn.Softmax(dim = 1)\n",
    ")                                                                                                                                             \n",
    "loss_fn = torch.nn.CrossEntropyLoss()                                                                                                                                      \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)                                                                                                         \n",
    "num_batch = len(data) // batch_size\n",
    "\n",
    "losses = []\n",
    "for t in range(1,steps+1):\n",
    "    i = t % num_batch\n",
    "    batch_loss = Variable(torch.zeros(1), requires_grad=True)\n",
    "    #num_road = 0\n",
    "    y_pred_list = []\n",
    "    y_list = []\n",
    "    for path in data[i*batch_size:(i+1)*batch_size]:\n",
    "        print(i*batch_size, (i+1)*batch_size)\n",
    "        x,y = training_set(path, graph)                                                                                                                                          \n",
    "        y_pred = model(x)  \n",
    "        loss = loss_fn(y_pred, y) # * len(y) # TODO: truancate y_pred, Contatinate\n",
    "        batch_loss = batch_loss + loss\n",
    "        #num_road = num_road + len(y)\n",
    "        #batch_loss /= num_road\n",
    "    if t%1000 == 0:\n",
    "        print(t, batch_loss.data[0])\n",
    "    #print(batch_loss.data[0])\n",
    "    losses.append(batch_loss.data[0])\n",
    "    if i == 0:\n",
    "        np.random.shuffle(data)\n",
    "    optimizer.zero_grad()                                                                                                                                                  \n",
    "    batch_loss.backward()                                                                                                                                                        \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = training_set(data[4], graph)\n",
    "print(inv_one_hot(x.data.numpy()))\n",
    "#print(mk_pd.predict([]))\n",
    "print(graph.neighbors(13))\n",
    "print(model(x)[0][19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)[0,:].data[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(prob,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight(prob, cut):\n",
    "    a,b = prob.size()\n",
    "    for i in range(len(cut)):\n",
    "        n = cut[i]\n",
    "        if n < b:\n",
    "            prob[i,n:] = 0\n",
    "            prob[i, :n] = prob[i, :n] / torch.sum(prob[i, :n])\n",
    "    return prob\n",
    "\n",
    "def count_exit(path):\n",
    "    return [len(graph.neighbors(i)) for i in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = F.softmax(autograd.Variable(torch.randn(2, 3)), dim = 1)\n",
    "cut = [1,2]\n",
    "reweight(prob,cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {0:{1:1, 2:4}, 1:{0:1,2:1}}\n",
    "np.array(list(table[0])) / sum(list(table[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(self, data, batch_size = 50, learning_rate = 0.001, steps = 10000):                                                                                                  \n",
    "data = random_walk_data(graph, 10000,go_back = True)\n",
    "\n",
    "batch_size = 50\n",
    "learning_rate = 0.001 \n",
    "steps = 1000\n",
    "D_in, D_hidden, D_out = len(graph.nodes), 100, 4                                                                                                                      \n",
    "\n",
    "model = torch.nn.Sequential(                                                                                                                                               \n",
    "    torch.nn.Linear(D_in, D_hidden),                                                                                                                                       \n",
    "    torch.nn.ReLU(),                                                                                                                                                       \n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    "    torch.nn.Softmax(dim = 1)\n",
    ")\n",
    "\n",
    "    \n",
    "loss_fn = torch.nn.CrossEntropyLoss()                                                                                                                                      \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)                                                                                                         \n",
    "num_batch = len(data) // batch_size\n",
    "\n",
    "losses = []\n",
    "for t in range(1,steps+1):\n",
    "    if t%100 == 0:\n",
    "        print(t)\n",
    "    i = t % num_batch\n",
    "    data_i = data[i*batch_size:(i+1)*batch_size]\n",
    "    batch_loss = Variable(torch.zeros(1), requires_grad=True)\n",
    "    num_road = 0\n",
    "    y_pred_list = []\n",
    "    y_list = []\n",
    "    for path in data_i:\n",
    "        num_exit = count_exit(path)[:-1]\n",
    "        x,y = training_set(path, graph)                                                                                                                                          \n",
    "        y_pred = reweight(model(x),num_exit)\n",
    "        y_list.append(y)\n",
    "        y_pred_list.append(y_pred)\n",
    "        \n",
    "    batch_loss = loss_fn(torch.cat(y_pred_list), torch.cat(y_list))\n",
    "        #num_road = num_road + len(y)\n",
    "    #batch_loss /= num_road\n",
    "    losses.append(batch_loss.data[0])\n",
    "    if i == 0:\n",
    "        print(batch_loss.data[0])\n",
    "        np.random.shuffle(data)\n",
    "    optimizer.zero_grad()                                                                                                                                                  \n",
    "    batch_loss.backward()                                                                                                                                                        \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = training_set(path, graph)\n",
    "path = [len(graph.neighbors(inv_one_hot(r)))  for r in x.data.numpy()]\n",
    "print(path)\n",
    "l = [len(graph.neighbors(r))  for r in path]\n",
    "y_  = model(x)\n",
    "#print(y_)\n",
    "\n",
    "def f(y_, l):\n",
    "    for i in range(len(l)):\n",
    "        print(y_[i,:l[i]]) #=  y_[i,:[i]] / torch.sum(y_[i,:[i]])\n",
    "        print(y_[i,l[i]:]) #= 0\n",
    "f(y_,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batch, D_in, D_hidden, D_out = 50, 24, 100, 4\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    "    torch.nn.Softmax(dim = 1),\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 0.002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "L = 0\n",
    "n = 0\n",
    "pts = {'x':[], 'y':[]}\n",
    "for t in range(1,40001):\n",
    "    x,y = random_training_set()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    L += sum(loss.data.numpy())\n",
    "    n += len(loss.data.numpy())\n",
    "    if t%1000 == 0:\n",
    "        pts['x'].append(t)\n",
    "        pts['y'].append(L/n)\n",
    "        print(t, L/n)\n",
    "        L = 0\n",
    "        n = 0\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pts['x'],pts['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batch, D_in, D_hidden, D_out = 50, 24, 100, 24\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    "    torch.nn.Softmax(dim = 1),\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 0.002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "L = 0\n",
    "n = 0\n",
    "pts = {'x':[], 'y':[]}\n",
    "for t in range(1,40001):\n",
    "    x,y = random_training_set_with_road_tar()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    L += sum(loss.data.numpy())\n",
    "    n += len(loss.data.numpy())\n",
    "    if t%1000 == 0:\n",
    "        pts['x'].append(t)\n",
    "        pts['y'].append(L/n)\n",
    "        print(t, L/n)\n",
    "        L = 0\n",
    "        n = 0\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = random_training_set_with_road_tar()\n",
    "print([inv_one_hot(r) for r in x.data.numpy()])\n",
    "np.concatenate([model(x)[0,:].data.numpy().reshape(24,1), \n",
    "                np.array(range(0,24)).reshape(24,1)],\n",
    "               axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_training_pair2(path):\n",
    "    return path[1:], path[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_next.view(-1,1).repeat(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0    \n",
    "print('x:', x[i],'nb:', graph.neighbors(x[i]))\n",
    "nb = graph.neighbors(x[i])\n",
    "W_list = [W[j].view(-1,1) for j in nb]\n",
    "Ws = torch.cat(W_list, dim = 1)\n",
    "W_next = model(W[x[0]])\n",
    "#print('W_next:', W_next)\n",
    "#print(W_next.size(), Ws.size())\n",
    "W_next_rep = W_next.view(-1,1).repeat(1,Ws.size()[1])\n",
    "#print(W_next_rep.size(), Ws.size())\n",
    "logit = torch.sum(W_next_rep*Ws, 0)\n",
    "prob = F.softmax(logit, dim = 0)\n",
    "print(prob)\n",
    "# loss = -torch.log(prob[np.where(nb == y[i])])\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(to_Variable([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_training_pair2(path):\n",
    "    return path[1:], path[:-1]\n",
    "\n",
    "def get_prob(x):\n",
    "    nb = graph.neighbors(x)\n",
    "    W_list = [W[j].view(-1,1) for j in nb]\n",
    "    Ws = torch.cat(W_list, dim = 1)\n",
    "    W_next = model(W[x])\n",
    "    #print('W_next:', W_next)\n",
    "    #print(W_next.size(), Ws.size())\n",
    "    W_next_rep = W_next.view(-1,1).repeat(1,Ws.size()[1])\n",
    "    #print(W_next_rep.size(), Ws.size())\n",
    "    logit = torch.sum(W_next_rep*Ws, 0)\n",
    "    prob = F.softmax(logit, dim = 0)\n",
    "    return prob\n",
    "\n",
    "def get_loss(x, y):\n",
    "    nb = graph.neighbors(x)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    W_list = [W[j].view(-1,1) for j in nb]\n",
    "    Ws = torch.cat(W_list, dim = 1)\n",
    "    W_next = model(W[x])\n",
    "    #print('W_next:', W_next)\n",
    "    #print(W_next.size(), Ws.size())\n",
    "    W_next_rep = W_next.view(-1,1).repeat(1,Ws.size()[1])\n",
    "    #print(W_next_rep.size(), Ws.size())\n",
    "    logit = torch.sum(W_next_rep*Ws, 0)\n",
    "    prob = F.softmax(logit, dim = 0)\n",
    "    loss = -torch.log(prob[np.where(nb == y)])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.56326079]\n",
      "[ 1.31235385]\n",
      "[ 1.25859869]\n",
      "[ 1.2124157]\n",
      "[ 1.19315529]\n",
      "[ 1.2256844]\n",
      "[ 1.21445966]\n",
      "[ 1.21762311]\n",
      "[ 1.16609609]\n",
      "[ 1.20807862]\n",
      "[ 1.20850778]\n",
      "[ 1.18176556]\n",
      "[ 1.19595766]\n",
      "[ 1.17643952]\n",
      "[ 1.1663599]\n",
      "[ 1.20453811]\n",
      "[ 1.18325412]\n",
      "[ 1.20353794]\n",
      "[ 1.17199945]\n",
      "[ 1.20330572]\n",
      "[ 1.1776619]\n",
      "[ 1.19055784]\n",
      "[ 1.17359149]\n",
      "[ 1.16179502]\n",
      "[ 1.17043316]\n",
      "[ 1.17666602]\n",
      "[ 1.19944215]\n",
      "[ 1.19959748]\n",
      "[ 1.19619]\n",
      "[ 1.20277858]\n",
      "[ 1.17322242]\n",
      "[ 1.17391276]\n",
      "[ 1.1822176]\n",
      "[ 1.1886605]\n",
      "[ 1.17198694]\n",
      "[ 1.16102064]\n",
      "[ 1.1778332]\n",
      "[ 1.16167629]\n",
      "[ 1.16876709]\n",
      "[ 1.17989445]\n",
      "[ 1.18759549]\n",
      "[ 1.18333387]\n",
      "[ 1.16917992]\n",
      "[ 1.1560781]\n",
      "[ 1.18768859]\n",
      "[ 1.17856967]\n",
      "[ 1.19322693]\n",
      "[ 1.19603515]\n",
      "[ 1.18229485]\n",
      "[ 1.17411816]\n",
      "[ 1.20339155]\n",
      "[ 1.17641711]\n",
      "[ 1.17005301]\n",
      "[ 1.18375981]\n",
      "[ 1.19276798]\n",
      "[ 1.19355595]\n",
      "[ 1.19308186]\n",
      "[ 1.1804738]\n",
      "[ 1.19791722]\n",
      "[ 1.1532526]\n",
      "[ 1.17205274]\n",
      "[ 1.16130722]\n",
      "[ 1.17824352]\n",
      "[ 1.19342017]\n",
      "[ 1.17489231]\n",
      "[ 1.1781677]\n",
      "[ 1.18670785]\n",
      "[ 1.18885505]\n",
      "[ 1.19412899]\n",
      "[ 1.17012846]\n",
      "[ 1.16618764]\n",
      "[ 1.16344428]\n",
      "[ 1.1902138]\n",
      "[ 1.18430042]\n",
      "[ 1.17048156]\n",
      "[ 1.18741488]\n",
      "[ 1.18165278]\n",
      "[ 1.17214119]\n",
      "[ 1.18094325]\n",
      "[ 1.14766645]\n",
      "[ 1.18396819]\n",
      "[ 1.16636646]\n",
      "[ 1.23116291]\n",
      "[ 1.16683233]\n",
      "[ 1.16026461]\n",
      "[ 1.18902385]\n",
      "[ 1.18493533]\n",
      "[ 1.2004118]\n",
      "[ 1.19524848]\n",
      "[ 1.17934585]\n",
      "[ 1.17291999]\n",
      "[ 1.17901564]\n",
      "[ 1.19046509]\n",
      "[ 1.17045701]\n",
      "[ 1.16718435]\n",
      "[ 1.15816033]\n",
      "[ 1.19383776]\n",
      "[ 1.14075255]\n",
      "[ 1.171332]\n",
      "[ 1.18248093]\n",
      "[ 1.19676638]\n",
      "[ 1.15465379]\n",
      "[ 1.17826462]\n",
      "[ 1.1826669]\n",
      "[ 1.173931]\n",
      "[ 1.165797]\n",
      "[ 1.16336036]\n",
      "[ 1.20605183]\n",
      "[ 1.1750567]\n",
      "[ 1.16709101]\n",
      "[ 1.17651236]\n",
      "[ 1.16103768]\n",
      "[ 1.19106793]\n",
      "[ 1.15895212]\n",
      "[ 1.1760602]\n",
      "[ 1.18610656]\n",
      "[ 1.18608463]\n",
      "[ 1.19194508]\n",
      "[ 1.15134013]\n",
      "[ 1.19234383]\n",
      "[ 1.16771817]\n",
      "[ 1.17354357]\n",
      "[ 1.17988098]\n",
      "[ 1.16623497]\n",
      "[ 1.2033323]\n",
      "[ 1.19475055]\n",
      "[ 1.1606909]\n",
      "[ 1.17899358]\n",
      "[ 1.1856432]\n",
      "[ 1.16151512]\n",
      "[ 1.1781168]\n",
      "[ 1.16881073]\n",
      "[ 1.18668628]\n",
      "[ 1.19016564]\n",
      "[ 1.18980896]\n",
      "[ 1.17400527]\n",
      "[ 1.15636456]\n",
      "[ 1.1506269]\n",
      "[ 1.15812469]\n",
      "[ 1.21139324]\n",
      "[ 1.18560231]\n",
      "[ 1.16911411]\n",
      "[ 1.18930697]\n",
      "[ 1.18598318]\n",
      "[ 1.18542194]\n",
      "[ 1.18149984]\n",
      "[ 1.16201425]\n",
      "[ 1.17251432]\n",
      "[ 1.19479001]\n",
      "[ 1.18483758]\n",
      "[ 1.19401681]\n",
      "[ 1.18606639]\n",
      "[ 1.17696214]\n",
      "[ 1.19309103]\n",
      "[ 1.16775155]\n",
      "[ 1.18259096]\n",
      "[ 1.17376184]\n",
      "[ 1.16946328]\n",
      "[ 1.17986929]\n",
      "[ 1.17408967]\n",
      "[ 1.18012321]\n",
      "[ 1.18189502]\n",
      "[ 1.1471076]\n",
      "[ 1.15867829]\n",
      "[ 1.17893898]\n",
      "[ 1.16701961]\n",
      "[ 1.19239306]\n",
      "[ 1.18498349]\n",
      "[ 1.17700517]\n",
      "[ 1.15323675]\n",
      "[ 1.19403028]\n",
      "[ 1.17704785]\n",
      "[ 1.15521789]\n",
      "[ 1.19471931]\n",
      "[ 1.16385162]\n",
      "[ 1.1722914]\n",
      "[ 1.16485393]\n",
      "[ 1.16149461]\n",
      "[ 1.20320344]\n",
      "[ 1.16923738]\n",
      "[ 1.19209921]\n",
      "[ 1.16670406]\n",
      "[ 1.17240298]\n",
      "[ 1.18072331]\n",
      "[ 1.16742587]\n",
      "[ 1.16925311]\n",
      "[ 1.16595984]\n",
      "[ 1.19890463]\n",
      "[ 1.15812302]\n",
      "[ 1.20534146]\n",
      "[ 1.1919564]\n",
      "[ 1.17755413]\n",
      "[ 1.18248153]\n",
      "[ 1.1589731]\n",
      "[ 1.217785]\n",
      "[ 1.19910455]\n",
      "[ 1.16527188]\n",
      "[ 1.14316142]\n",
      "[ 1.18822098]\n",
      "[ 1.18151617]\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "\n",
    "data = random_walk_data(graph,10000)\n",
    "\n",
    "im_D = 5\n",
    "N = 24\n",
    "h_D = 100\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "W = Variable(torch.randn(N,im_D).type(dtype), requires_grad=True)\n",
    "model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(im_D, h_D),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(h_D, im_D),\n",
    "    )\n",
    "for k in range(len(data)):\n",
    "    path = data[k]\n",
    "    x,y = path_to_training_pair2(path)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + [W], lr=0.001)\n",
    "    for i in range(len(x)):\n",
    "        loss = get_loss(x[i], y[i])\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "    if k%50 == 0:\n",
    "        batch_loss = torch.sum(torch.cat(loss_list)) / len(loss_list)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list = []\n",
    "        print(batch_loss.data.numpy())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(3) /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1295891838769119"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_loss():\n",
    "    s = 0\n",
    "    for i in range(24):\n",
    "        p_ = get_prob(i).data.numpy()\n",
    "        n = len(get_prob(i))\n",
    "        s += -np.sum(np.ones(n) /n * np.log(p_))\n",
    "    return s/24\n",
    "log_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esay Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = [1/3,1/3,1/3]\n",
    "y = [1/3,1/3,1/3]\n",
    "-np.sum(y*np.log(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "x.backward()\n",
    "optimizer.step()\n",
    "x.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
