{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import Data\n",
    "import utils\n",
    "import Predictors, Road_Graph, Neural_Network\n",
    "imp.reload(Data)\n",
    "imp.reload(Neural_Network)\n",
    "imp.reload(utils)\n",
    "imp.reload(Predictors)\n",
    "imp.reload(Road_Graph)\n",
    "\n",
    "from Road_Graph import *\n",
    "from Predictors import *\n",
    "from Data import *\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch import nn, autograd\n",
    "import torch.nn.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([16, 11, 23]), Variable containing:\n",
       "  0.0016\n",
       "  0.5192\n",
       "  0.4792\n",
       " [torch.FloatTensor of size 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_pd.predict([0,1,2,8,14,15,16,17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we focus on a simple road map (rep as a graph below), and generate random path on it.\n",
    "<img src=\"../img/naive_road.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "graph_dict = graph_dict.fromkeys(range(24))\n",
    "edges = [(i, i+1) for i in range(5)] \\\n",
    "        + [(i+6, i+7) for i in range(5)]\\\n",
    "                        +[(i+12, i+13) for i in range(5)]\\\n",
    "                     + [(i+18, i+19) for i in range(5)]\\\n",
    "                     + [(0,6),(6,12), (12,18)]\\\n",
    "                     + [(pair[0]+1, pair[1]+1) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+2, pair[1]+2) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+3, pair[1]+3) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+4, pair[1]+4) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+5, pair[1]+5) for pair in [(0,6),(6,12), (12,18)]]\n",
    "for i,j in edges:\n",
    "    if not graph_dict[i]:\n",
    "        graph_dict[i] = [j]\n",
    "    else:\n",
    "        graph_dict[i].append(j)\n",
    "    if not graph_dict[j]:\n",
    "        graph_dict[j] = [i]\n",
    "    else:\n",
    "        graph_dict[j].append(i)\n",
    "\n",
    "graph = Road_Graph(graph_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_training_pair2(path):\n",
    "    return path[1:], path[:-1]\n",
    "\n",
    "def get_prob(x):\n",
    "    nb = graph.neighbors(x)\n",
    "    W_list = [W[j].view(-1,1) for j in nb]\n",
    "    Ws = torch.cat(W_list, dim = 1)\n",
    "    W_next = model(W[x])\n",
    "    #print('W_next:', W_next)\n",
    "    #print(W_next.size(), Ws.size())\n",
    "    W_next_rep = W_next.view(-1,1).repeat(1,Ws.size()[1])\n",
    "    #print(W_next_rep.size(), Ws.size())\n",
    "    logit = torch.sum(W_next_rep*Ws, 0)\n",
    "    prob = F.softmax(logit, dim = 0)\n",
    "    return prob\n",
    "\n",
    "def get_loss(x, y):\n",
    "    nb = graph.neighbors(x)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    W_list = [W[j].view(-1,1) for j in nb]\n",
    "    Ws = torch.cat(W_list, dim = 1)\n",
    "    W_next = model(W[x])\n",
    "    #print('W_next:', W_next)\n",
    "    #print(W_next.size(), Ws.size())\n",
    "    W_next_rep = W_next.view(-1,1).repeat(1,Ws.size()[1])\n",
    "    #print(W_next_rep.size(), Ws.size())\n",
    "    logit = torch.sum(W_next_rep*Ws, 0)\n",
    "    prob = F.softmax(logit, dim = 0)\n",
    "    loss = -torch.log(prob[np.where(nb == y)])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "\n",
    "data = random_walk_data(graph,10000)\n",
    "\n",
    "im_D = 5\n",
    "N = 24\n",
    "h_D = 100\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "W = Variable(torch.randn(N,im_D).type(dtype), requires_grad=True)\n",
    "model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(im_D, h_D),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(h_D, im_D),\n",
    "    )\n",
    "for k in range(len(data)):\n",
    "    path = data[k]\n",
    "    x,y = path_to_training_pair2(path)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + [W], lr=0.001) # remove [W] for a fixed random embeding\n",
    "    for i in range(len(x)):\n",
    "        loss = get_loss(x[i], y[i])\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "    if k%50 == 0:\n",
    "        batch_loss = torch.sum(torch.cat(loss_list)) / len(loss_list)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list = []\n",
    "        print(batch_loss.data.numpy())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_log_loss():\n",
    "    s = 0\n",
    "    for i in range(24):\n",
    "        n = len(graph.neighbors(i)) - 1\n",
    "        s += -np.sum(np.ones(n) /n * np.log(np.ones(n)/n))\n",
    "    return s/24\n",
    "opt_log_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_log_loss():\n",
    "    s = 0\n",
    "    for i in range(24):\n",
    "        p_ = get_prob(i).data.numpy()\n",
    "        n = len(get_prob(i))\n",
    "        s += -np.sum(np.ones(n) /n * np.log(np.ones(n)/n))\n",
    "    return s/24\n",
    "opt_log_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss():\n",
    "    s = 0\n",
    "    for i in range(24):\n",
    "        p_ = get_prob(i).data.numpy()\n",
    "        n = len(get_prob(i))\n",
    "        s += -np.sum(np.ones(n) /n * np.log(p_))\n",
    "    return s/24\n",
    "log_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss():\n",
    "    s = 0\n",
    "    for i in range(24):\n",
    "        p_ = get_prob(i).data.numpy()\n",
    "        n = len(get_prob(i))\n",
    "        s += -np.sum(np.ones(n) /n * np.log(p_))\n",
    "    return s/24\n",
    "log_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prob(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hist Dependent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layers)\n",
    "        self.lin = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, inp, hidden):\n",
    "        output, hidden = self.gru(inp.view(1, 1, self.input_size), hidden)\n",
    "        output = self.lin(output) \n",
    "        return output, hidden        \n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_loss(path):\n",
    "    h = model.init_hidden()\n",
    "    output_list = []\n",
    "    for i in path:\n",
    "        o, h = model(W[i], h)\n",
    "        output_list.append(o)\n",
    "    loss_list = []\n",
    "    for i in range(len(path) - 1):\n",
    "        x = path[i]\n",
    "        y = path[i+1]\n",
    "        nb = graph.neighbors(x)\n",
    "        W_list = [W[j].view(-1,1) for j in nb]\n",
    "        Ws = torch.cat(W_list, dim = 1)\n",
    "        W_next = output_list[i]\n",
    "        W_next_rep = W_next.view(-1,1).repeat(1,Ws.size()[1])\n",
    "        logit = torch.sum(W_next_rep*Ws, 0)\n",
    "        prob = F.softmax(logit, dim = 0)\n",
    "        loss_list.append(-torch.log(prob[np.where(nb == y)]))\n",
    "    return torch.sum(torch.cat(loss_list))\n",
    "\n",
    "def get_prob(path):\n",
    "    h = model.init_hidden()\n",
    "    for i in path:\n",
    "        o, h = model(W[i], h)\n",
    "    x = path[-1]\n",
    "    nb = graph.neighbors(x)\n",
    "    W_list = [W[j].view(-1,1) for j in nb]\n",
    "    Ws = torch.cat(W_list, dim = 1)\n",
    "    W_next = o\n",
    "    W_next_rep = W_next.view(-1,1).repeat(1,Ws.size()[1])\n",
    "    logit = torch.sum(W_next_rep*Ws, 0)\n",
    "    prob = F.softmax(logit, dim = 0)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = random_walk_data(graph, 10000, go_back=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_D = 50\n",
    "N = 24\n",
    "h_D = 100\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "\n",
    "W = Variable(torch.randn(N,im_D).type(dtype), requires_grad=True)\n",
    "model = RNN(im_D, h_D, im_D)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # remove [W] for a fixed random embeding\n",
    "\n",
    "loss_list = []\n",
    "s = 0\n",
    "for k in range(len(data)):\n",
    "    path = data[k]\n",
    "    loss = path_loss(path)\n",
    "    loss_list.append(loss)    \n",
    "    s += len(path)\n",
    "    if k%50 == 0:\n",
    "        batch_loss = torch.sum(torch.cat(loss_list)) / s\n",
    "        s = 0\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list = []\n",
    "        print(batch_loss.data.numpy())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prob([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulized Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = random_walk_data(graph, 200000, go_back=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_pd = RNN_Predictor(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 1000 steps: 1.0716890686800273\n",
      "Loss after 2000 steps: 0.9357152714814334\n",
      "Loss after 3000 steps: 0.8858575648135681\n",
      "Loss after 4000 steps: 0.8581178708161664\n",
      "Loss after 5000 steps: 0.8427021602890676\n",
      "Loss after 6000 steps: 0.8225637010670827\n",
      "Loss after 7000 steps: 0.8267547350602289\n",
      "Loss after 8000 steps: 0.8125967688591874\n",
      "Loss after 9000 steps: 0.8079272696829592\n",
      "Loss after 10000 steps: 0.8052598047192026\n",
      "Loss after 11000 steps: 0.7995027268831563\n",
      "Loss after 12000 steps: 0.7975608054380755\n",
      "Loss after 13000 steps: 0.7942090446908708\n",
      "Loss after 14000 steps: 0.796186477256597\n",
      "Loss after 15000 steps: 0.7865377018104216\n",
      "Loss after 16000 steps: 0.7854723164005479\n",
      "Loss after 17000 steps: 0.7901354226348591\n",
      "Loss after 18000 steps: 0.7879877107814146\n",
      "Loss after 19000 steps: 0.777418080370983\n",
      "Loss after 20000 steps: 0.7839760389860486\n",
      "Loss after 21000 steps: 0.7833162295678966\n",
      "Loss after 22000 steps: 0.7731230085421898\n",
      "Loss after 23000 steps: 0.7744686836909603\n",
      "Loss after 24000 steps: 0.7849299093403949\n",
      "Loss after 25000 steps: 0.7824060347046252\n",
      "Loss after 26000 steps: 0.775677866412003\n",
      "Loss after 27000 steps: 0.7816756667593199\n",
      "Loss after 28000 steps: 0.7776600522197166\n",
      "Loss after 29000 steps: 0.7743203448616525\n",
      "Loss after 30000 steps: 0.7701675379255092\n",
      "Loss after 31000 steps: 0.7801991859007287\n",
      "Loss after 32000 steps: 0.7816522354079319\n",
      "Loss after 33000 steps: 0.7767352969533978\n",
      "Loss after 34000 steps: 0.7675260812834462\n",
      "Loss after 35000 steps: 0.7643727307677326\n",
      "Loss after 36000 steps: 0.7689613896045184\n",
      "Loss after 37000 steps: 0.7702785261670022\n",
      "Loss after 38000 steps: 0.7707240209408938\n",
      "Loss after 39000 steps: 0.7713176067748412\n",
      "Loss after 40000 steps: 0.771746748313445\n",
      "Loss after 41000 steps: 0.7739054581552983\n",
      "Loss after 42000 steps: 0.772874451950971\n",
      "Loss after 43000 steps: 0.7616818171726149\n",
      "Loss after 44000 steps: 0.7678791837080421\n",
      "Loss after 45000 steps: 0.7694137597284623\n",
      "Loss after 46000 steps: 0.7659665281121387\n",
      "Loss after 47000 steps: 0.7652529571651314\n",
      "Loss after 48000 steps: 0.7663958358087044\n",
      "Loss after 49000 steps: 0.7659067463978128\n",
      "Loss after 50000 steps: 0.7709477677133348\n",
      "Loss after 51000 steps: 0.7646948950182951\n",
      "Loss after 52000 steps: 0.7692378723284083\n",
      "Loss after 53000 steps: 0.7656957336271514\n",
      "Loss after 54000 steps: 0.769445549690445\n",
      "Loss after 55000 steps: 0.7632057513764317\n",
      "Loss after 56000 steps: 0.7660147973581733\n",
      "Loss after 57000 steps: 0.761125459837388\n",
      "Loss after 58000 steps: 0.7700415240507231\n",
      "Loss after 59000 steps: 0.7669842426759039\n",
      "Loss after 60000 steps: 0.7673075730354966\n",
      "Loss after 61000 steps: 0.7621093840509748\n",
      "Loss after 62000 steps: 0.7626756486897661\n",
      "Loss after 63000 steps: 0.7604422937765862\n",
      "Loss after 64000 steps: 0.7611332953701162\n",
      "Loss after 65000 steps: 0.7694160208012992\n",
      "Loss after 66000 steps: 0.7626086587443828\n",
      "Loss after 67000 steps: 0.7668965337763326\n",
      "Loss after 68000 steps: 0.7694750907220832\n",
      "Loss after 69000 steps: 0.7626869696028092\n",
      "Loss after 70000 steps: 0.7587763457024235\n",
      "Loss after 71000 steps: 0.7656981999136234\n",
      "Loss after 72000 steps: 0.7660723127394917\n",
      "Loss after 73000 steps: 0.7584638787986561\n",
      "Loss after 74000 steps: 0.7617917215803837\n",
      "Loss after 75000 steps: 0.760649258416484\n",
      "Loss after 76000 steps: 0.7654357149739972\n",
      "Loss after 77000 steps: 0.762868647753259\n",
      "Loss after 78000 steps: 0.7666222223387795\n",
      "Loss after 79000 steps: 0.7611991995763495\n",
      "Loss after 80000 steps: 0.7614784023814731\n",
      "Loss after 81000 steps: 0.7553411046366283\n",
      "Loss after 82000 steps: 0.7621507592384871\n",
      "Loss after 83000 steps: 0.7577227330802941\n",
      "Loss after 84000 steps: 0.7628937867495561\n",
      "Loss after 85000 steps: 0.7656055545582048\n",
      "Loss after 86000 steps: 0.7586753039518741\n",
      "Loss after 87000 steps: 0.7624812762955463\n",
      "Loss after 88000 steps: 0.7635383353471316\n",
      "Loss after 89000 steps: 0.7624329318844735\n",
      "Loss after 90000 steps: 0.7651588693030764\n",
      "Loss after 91000 steps: 0.7665500829620787\n",
      "Loss after 92000 steps: 0.7629911020670314\n",
      "Loss after 93000 steps: 0.7600261413995609\n",
      "Loss after 94000 steps: 0.7625768980025398\n",
      "Loss after 95000 steps: 0.7694082451305597\n",
      "Loss after 96000 steps: 0.7571239962105826\n",
      "Loss after 97000 steps: 0.763227974856886\n",
      "Loss after 98000 steps: 0.7622295055783647\n",
      "Loss after 99000 steps: 0.7643488695233235\n",
      "Loss after 100000 steps: 0.7636428517943346\n",
      "Loss after 101000 steps: 0.7613590485049345\n",
      "Loss after 102000 steps: 0.7633247500230005\n",
      "Loss after 103000 steps: 0.7591716103818511\n",
      "Loss after 104000 steps: 0.7616378877031518\n",
      "Loss after 105000 steps: 0.7611797045041704\n",
      "Loss after 106000 steps: 0.7585393540970883\n",
      "Loss after 107000 steps: 0.7627408746709793\n",
      "Loss after 108000 steps: 0.7628846688223854\n",
      "Loss after 109000 steps: 0.7591034857612343\n",
      "Loss after 110000 steps: 0.7569676653957237\n",
      "Loss after 111000 steps: 0.7623261687967343\n",
      "Loss after 112000 steps: 0.7639518439829902\n",
      "Loss after 113000 steps: 0.7605355141195224\n",
      "Loss after 114000 steps: 0.7640092056058245\n",
      "Loss after 115000 steps: 0.7611422624366877\n",
      "Loss after 116000 steps: 0.7637001221781857\n",
      "Loss after 117000 steps: 0.7710334954220468\n",
      "Loss after 118000 steps: 0.7573683194848282\n",
      "Loss after 119000 steps: 0.7590524994121064\n",
      "Loss after 120000 steps: 0.7609403091245475\n",
      "Loss after 121000 steps: 0.7586723989500765\n",
      "Loss after 122000 steps: 0.7602040489215588\n",
      "Loss after 123000 steps: 0.7623294510967761\n",
      "Loss after 124000 steps: 0.7577892240178029\n",
      "Loss after 125000 steps: 0.759525803599038\n",
      "Loss after 126000 steps: 0.7594594963967709\n",
      "Loss after 127000 steps: 0.7635937832454418\n",
      "Loss after 128000 steps: 0.7647564538597633\n",
      "Loss after 129000 steps: 0.7629326910241431\n",
      "Loss after 130000 steps: 0.7659455480641642\n",
      "Loss after 131000 steps: 0.7603241735693405\n",
      "Loss after 132000 steps: 0.763065616975073\n",
      "Loss after 133000 steps: 0.7596817240656881\n",
      "Loss after 134000 steps: 0.7657675706186949\n",
      "Loss after 135000 steps: 0.7642638322868126\n",
      "Loss after 136000 steps: 0.7611653653701399\n",
      "Loss after 137000 steps: 0.759127435817086\n",
      "Loss after 138000 steps: 0.7600050197808551\n",
      "Loss after 139000 steps: 0.7615421999366189\n",
      "Loss after 140000 steps: 0.760292791696026\n",
      "Loss after 141000 steps: 0.7635079729082662\n",
      "Loss after 142000 steps: 0.7617238275159033\n",
      "Loss after 143000 steps: 0.7632537396421387\n",
      "Loss after 144000 steps: 0.7589458683104032\n",
      "Loss after 145000 steps: 0.7623096231524277\n",
      "Loss after 146000 steps: 0.7577307794466425\n",
      "Loss after 147000 steps: 0.7616451118905059\n",
      "Loss after 148000 steps: 0.7593046553287419\n",
      "Loss after 149000 steps: 0.7669345754677355\n",
      "Loss after 150000 steps: 0.75394784872748\n",
      "Loss after 151000 steps: 0.7565683295136627\n",
      "Loss after 152000 steps: 0.7531671336463827\n",
      "Loss after 153000 steps: 0.7592449507548846\n",
      "Loss after 154000 steps: 0.7562803871367832\n",
      "Loss after 155000 steps: 0.7638184659576958\n",
      "Loss after 156000 steps: 0.7609921796466148\n",
      "Loss after 157000 steps: 0.7662815880804855\n",
      "Loss after 158000 steps: 0.7568956279376093\n",
      "Loss after 159000 steps: 0.7576383169301436\n",
      "Loss after 160000 steps: 0.7587512825511658\n",
      "Loss after 161000 steps: 0.7575399888392055\n",
      "Loss after 162000 steps: 0.7603266781807646\n",
      "Loss after 163000 steps: 0.7596386449718552\n",
      "Loss after 164000 steps: 0.7672479442159177\n",
      "Loss after 165000 steps: 0.7534538751591083\n",
      "Loss after 166000 steps: 0.7576622889860866\n",
      "Loss after 167000 steps: 0.7644585576483959\n",
      "Loss after 168000 steps: 0.7614396676421166\n",
      "Loss after 169000 steps: 0.7607911183309515\n",
      "Loss after 170000 steps: 0.758094576356213\n",
      "Loss after 171000 steps: 0.7567709307965216\n",
      "Loss after 172000 steps: 0.7629115641352392\n",
      "Loss after 173000 steps: 0.7580123218217265\n",
      "Loss after 174000 steps: 0.7624068819167803\n",
      "Loss after 175000 steps: 0.7644420489984806\n",
      "Loss after 176000 steps: 0.7590061935394868\n",
      "Loss after 177000 steps: 0.7633958787817291\n",
      "Loss after 178000 steps: 0.7599713033050166\n",
      "Loss after 179000 steps: 0.759562640746283\n",
      "Loss after 180000 steps: 0.7646542508036596\n",
      "Loss after 181000 steps: 0.7589934342598504\n",
      "Loss after 182000 steps: 0.7599041544858457\n",
      "Loss after 183000 steps: 0.7673261718257557\n",
      "Loss after 184000 steps: 0.7559575385572505\n",
      "Loss after 185000 steps: 0.7648894175589745\n",
      "Loss after 186000 steps: 0.761228495119871\n",
      "Loss after 187000 steps: 0.7598046697580836\n",
      "Loss after 188000 steps: 0.767686006916489\n",
      "Loss after 189000 steps: 0.7631186410318123\n",
      "Loss after 190000 steps: 0.7616098730845872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 191000 steps: 0.7606975305012229\n",
      "Loss after 192000 steps: 0.7591090473306544\n",
      "Loss after 193000 steps: 0.7575204856728606\n",
      "Loss after 194000 steps: 0.7578850564756866\n",
      "Loss after 195000 steps: 0.7610649959663642\n",
      "Loss after 196000 steps: 0.7546343855786607\n",
      "Loss after 197000 steps: 0.7627569611718199\n",
      "Loss after 198000 steps: 0.7577663744496338\n",
      "Loss after 199000 steps: 0.7594562984906723\n",
      "Loss after 200000 steps: 0.7572365541767043\n"
     ]
    }
   ],
   "source": [
    "rnn_pd.train(data,steps = 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 6]), Variable containing:\n",
       "  0.4912\n",
       "  0.5088\n",
       " [torch.FloatTensor of size 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_pd.predict([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal total cross_entropy loss is ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(to_Variable([0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
