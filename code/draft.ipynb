{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import Data\n",
    "import utils\n",
    "import Predictors, Road_Graph\n",
    "imp.reload(Data)\n",
    "imp.reload(utils)\n",
    "imp.reload(Predictors)\n",
    "imp.reload(Road_Graph)\n",
    "\n",
    "from Road_Graph import *\n",
    "from Predictors import *\n",
    "from Data import *\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch import nn, autograd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we focus on a simple road map (rep as a graph below), and generate random path on it.\n",
    "<img src=\"../img/naive_road.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the road graph as above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "graph_dict = graph_dict.fromkeys(range(24))\n",
    "edges = [(i, i+1) for i in range(5)] \\\n",
    "        + [(i+6, i+7) for i in range(5)]\\\n",
    "                        +[(i+12, i+13) for i in range(5)]\\\n",
    "                     + [(i+18, i+19) for i in range(5)]\\\n",
    "                     + [(0,6),(6,12), (12,18)]\\\n",
    "                     + [(pair[0]+1, pair[1]+1) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+2, pair[1]+2) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+3, pair[1]+3) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+4, pair[1]+4) for pair in [(0,6),(6,12), (12,18)]]\\\n",
    "                     + [(pair[0]+5, pair[1]+5) for pair in [(0,6),(6,12), (12,18)]]\n",
    "for i,j in edges:\n",
    "    if not graph_dict[i]:\n",
    "        graph_dict[i] = [j]\n",
    "    else:\n",
    "        graph_dict[i].append(j)\n",
    "    if not graph_dict[j]:\n",
    "        graph_dict[j] = [i]\n",
    "    else:\n",
    "        graph_dict[j].append(i)\n",
    "\n",
    "#display(graph_dict)\n",
    "graph = Road_Graph(graph_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = random_walk_data(graph, 10000,go_back = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_pd = Markov_Predictor(graph)\n",
    "mk_pd.train(data)\n",
    "nxt,prob = mk_pd.predict([0,1,2])\n",
    "print(nxt, prob)\n",
    "mk_pd.eval(random_walk_data(graph, 1000,go_back = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.neighbors([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = random_walk_data(graph, 10000, go_back = True)\n",
    "\n",
    "D_in, D_hidden, D_out = 24, 100, 24\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    "    torch.nn.Softmax(dim = 1),\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 0.002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "L = 0\n",
    "n = 0\n",
    "pts = {'x':[], 'y':[]}\n",
    "for t in range(1,20001):\n",
    "    path = data[t % len(data)]\n",
    "    x,y = path_to_training_pair(path, graph)\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    L += sum(loss.data.numpy())\n",
    "    n += len(loss.data.numpy())\n",
    "    if t%1000 == 0:\n",
    "        pts['x'].append(t)\n",
    "        pts['y'].append(L/n)\n",
    "        print(t, L/n)\n",
    "        L = 0\n",
    "        n = 0\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pts['x'],pts['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = training_set(data[0], graph)\n",
    "print(inv_one_hot(x.data.numpy()))\n",
    "print(graph.neighbors(11))\n",
    "print(model(x)[1][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_loss(path):\n",
    "    def lable(x_raw, y):\n",
    "        # x,y are road level\n",
    "        nb = graph.neighbors(x_raw)\n",
    "        z = y.data[0]\n",
    "        return to_Variable([np.where(np.array(nb) == z)[0][0]])\n",
    "    def pred(x_raw, y_):\n",
    "        nb = graph.neighbors(x_raw)\n",
    "        return F.softmax(torch.cat([y_[n] for n in nb]), dim=0)\n",
    "    path = data[1]\n",
    "    X_raw = path[:-1]\n",
    "    X,Y = path_to_training_pair(path,graph)\n",
    "    Y_ = model(X)\n",
    "    loss = []\n",
    "    for i in range(len(Y)):\n",
    "        x, y, y_, x_raw = X[i], Y[i], Y_[i], X_raw[i]\n",
    "        loss.append(F.cross_entropy(pred(x_raw,y_).view(1,-1),  lable(x_raw,y))) \n",
    "    return torch.sum(torch.cat(loss))\n",
    "\n",
    "def train_step(path):\n",
    "    loss = comp_loss(path)\n",
    "    optimizer.zero_grad()                                                                                                                                                  \n",
    "    loss.backward()                                                                                                                                                        \n",
    "    optimizer.step()\n",
    "def pred(x_raw):\n",
    "    nb = graph.neighbors(x_raw)\n",
    "    y_ = model(to_Variable(one_hot(x_raw)))\n",
    "    return nb, F.softmax(torch.cat([y_[n] for n in nb]), dim=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path\n",
    "X,Y = path_to_training_pair(path,graph)\n",
    "x,y = X[0],Y[0]\n",
    "inv_one_hot(x.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw = inv_one_hot(x.data.numpy())\n",
    "y_ = model(x[0])\n",
    "nb = graph.neighbors(x_raw)\n",
    "y_ = model(to_Variable(one_hot(x_raw)))\n",
    "print(nb, F.softmax(torch.cat([y_[n] for n in nb]), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x_raw):\n",
    "    nb = graph.neighbors(x_raw)\n",
    "    y_ = model(to_Variable(one_hot(x_raw, 24)))\n",
    "    return nb, F.softmax(torch.cat([y_[n] for n in nb]), dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = random_walk_data(graph, 10000, go_back = True)\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.002 \n",
    "steps = 20000\n",
    "D_in, D_hidden, D_out = len(graph.nodes), 100, len(graph.nodes)                                                                                                                 \n",
    "\n",
    "model = torch.nn.Sequential(                                                                                                                                               \n",
    "    torch.nn.Linear(D_in, D_hidden),                                                                                                                                       \n",
    "    torch.nn.ReLU(),                                                                                                                                                      \n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    ")                                                                                                                                             \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)                                                                                                         \n",
    "num_batch = len(data) // batch_size\n",
    "\n",
    "losses = []\n",
    "for t in range(1,steps+1):\n",
    "    i = t % num_batch\n",
    "    batch_loss = Variable(torch.zeros(1), requires_grad=True)\n",
    "    #num_road = 0\n",
    "    y_pred_list = []\n",
    "    y_list = []\n",
    "    for path in data[i*batch_size:(i+1)*batch_size]:\n",
    "        train_step(path)\n",
    "    if t%1000 == 0:\n",
    "        print(t)#, batch_loss.data[0])\n",
    "    #losses.append(batch_loss.data[0])\n",
    "    if i == 0:\n",
    "        np.random.shuffle(data)\n",
    "    #optimizer.zero_grad()                                                                                                                                                  \n",
    "    #loss.backward()                                                                                                                                                        \n",
    "    #optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = training_set(data[0], graph)\n",
    "print(inv_one_hot(x.data.numpy()))\n",
    "#print(mk_pd.predict([]))\n",
    "print(graph.neighbors(5))\n",
    "print(model(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(self, data, batch_size = 50, learning_rate = 0.001, steps = 10000):                                                                                                  \n",
    "data = random_walk_data(graph, 10000, go_back = True)\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.002 \n",
    "steps = 20000\n",
    "D_in, D_hidden, D_out = len(graph.nodes), 100, len(graph.nodes)                                                                                                                 \n",
    "\n",
    "model = torch.nn.Sequential(                                                                                                                                               \n",
    "    torch.nn.Linear(D_in, D_hidden),                                                                                                                                       \n",
    "    torch.nn.ReLU(),                                                                                                                                                      \n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    "    torch.nn.Softmax(dim = 1)\n",
    ")                                                                                                                                             \n",
    "loss_fn = torch.nn.CrossEntropyLoss()                                                                                                                                      \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)                                                                                                         \n",
    "num_batch = len(data) // batch_size\n",
    "\n",
    "losses = []\n",
    "for t in range(1,steps+1):\n",
    "    i = t % num_batch\n",
    "    batch_loss = Variable(torch.zeros(1), requires_grad=True)\n",
    "    #num_road = 0\n",
    "    y_pred_list = []\n",
    "    y_list = []\n",
    "    for path in data[i*batch_size:(i+1)*batch_size]:\n",
    "        print(i*batch_size, (i+1)*batch_size)\n",
    "        x,y = training_set(path, graph)                                                                                                                                          \n",
    "        y_pred = model(x)  \n",
    "        loss = loss_fn(y_pred, y) # * len(y) # TODO: truancate y_pred, Contatinate\n",
    "        batch_loss = batch_loss + loss\n",
    "        #num_road = num_road + len(y)\n",
    "        #batch_loss /= num_road\n",
    "    if t%1000 == 0:\n",
    "        print(t, batch_loss.data[0])\n",
    "    #print(batch_loss.data[0])\n",
    "    losses.append(batch_loss.data[0])\n",
    "    if i == 0:\n",
    "        np.random.shuffle(data)\n",
    "    optimizer.zero_grad()                                                                                                                                                  \n",
    "    batch_loss.backward()                                                                                                                                                        \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = training_set(data[4], graph)\n",
    "print(inv_one_hot(x.data.numpy()))\n",
    "#print(mk_pd.predict([]))\n",
    "print(graph.neighbors(13))\n",
    "print(model(x)[0][19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)[0,:].data[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(prob,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight(prob, cut):\n",
    "    a,b = prob.size()\n",
    "    for i in range(len(cut)):\n",
    "        n = cut[i]\n",
    "        if n < b:\n",
    "            prob[i,n:] = 0\n",
    "            prob[i, :n] = prob[i, :n] / torch.sum(prob[i, :n])\n",
    "    return prob\n",
    "\n",
    "def count_exit(path):\n",
    "    return [len(graph.neighbors(i)) for i in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = F.softmax(autograd.Variable(torch.randn(2, 3)), dim = 1)\n",
    "cut = [1,2]\n",
    "reweight(prob,cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {0:{1:1, 2:4}, 1:{0:1,2:1}}\n",
    "np.array(list(table[0])) / sum(list(table[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(self, data, batch_size = 50, learning_rate = 0.001, steps = 10000):                                                                                                  \n",
    "data = random_walk_data(graph, 10000,go_back = True)\n",
    "\n",
    "batch_size = 50\n",
    "learning_rate = 0.001 \n",
    "steps = 1000\n",
    "D_in, D_hidden, D_out = len(graph.nodes), 100, 4                                                                                                                      \n",
    "\n",
    "model = torch.nn.Sequential(                                                                                                                                               \n",
    "    torch.nn.Linear(D_in, D_hidden),                                                                                                                                       \n",
    "    torch.nn.ReLU(),                                                                                                                                                       \n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    "    torch.nn.Softmax(dim = 1)\n",
    ")\n",
    "\n",
    "    \n",
    "loss_fn = torch.nn.CrossEntropyLoss()                                                                                                                                      \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)                                                                                                         \n",
    "num_batch = len(data) // batch_size\n",
    "\n",
    "losses = []\n",
    "for t in range(1,steps+1):\n",
    "    if t%100 == 0:\n",
    "        print(t)\n",
    "    i = t % num_batch\n",
    "    data_i = data[i*batch_size:(i+1)*batch_size]\n",
    "    batch_loss = Variable(torch.zeros(1), requires_grad=True)\n",
    "    num_road = 0\n",
    "    y_pred_list = []\n",
    "    y_list = []\n",
    "    for path in data_i:\n",
    "        num_exit = count_exit(path)[:-1]\n",
    "        x,y = training_set(path, graph)                                                                                                                                          \n",
    "        y_pred = reweight(model(x),num_exit)\n",
    "        y_list.append(y)\n",
    "        y_pred_list.append(y_pred)\n",
    "        \n",
    "    batch_loss = loss_fn(torch.cat(y_pred_list), torch.cat(y_list))\n",
    "        #num_road = num_road + len(y)\n",
    "    #batch_loss /= num_road\n",
    "    losses.append(batch_loss.data[0])\n",
    "    if i == 0:\n",
    "        print(batch_loss.data[0])\n",
    "        np.random.shuffle(data)\n",
    "    optimizer.zero_grad()                                                                                                                                                  \n",
    "    batch_loss.backward()                                                                                                                                                        \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = training_set(path, graph)\n",
    "path = [len(graph.neighbors(inv_one_hot(r)))  for r in x.data.numpy()]\n",
    "print(path)\n",
    "l = [len(graph.neighbors(r))  for r in path]\n",
    "y_  = model(x)\n",
    "#print(y_)\n",
    "\n",
    "def f(y_, l):\n",
    "    for i in range(len(l)):\n",
    "        print(y_[i,:l[i]]) #=  y_[i,:[i]] / torch.sum(y_[i,:[i]])\n",
    "        print(y_[i,l[i]:]) #= 0\n",
    "f(y_,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batch, D_in, D_hidden, D_out = 50, 24, 100, 4\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    "    torch.nn.Softmax(dim = 1),\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 0.002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "L = 0\n",
    "n = 0\n",
    "pts = {'x':[], 'y':[]}\n",
    "for t in range(1,40001):\n",
    "    x,y = random_training_set()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    L += sum(loss.data.numpy())\n",
    "    n += len(loss.data.numpy())\n",
    "    if t%1000 == 0:\n",
    "        pts['x'].append(t)\n",
    "        pts['y'].append(L/n)\n",
    "        print(t, L/n)\n",
    "        L = 0\n",
    "        n = 0\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pts['x'],pts['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batch, D_in, D_hidden, D_out = 50, 24, 100, 24\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(D_hidden, D_out),\n",
    "    torch.nn.Softmax(dim = 1),\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 0.002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "L = 0\n",
    "n = 0\n",
    "pts = {'x':[], 'y':[]}\n",
    "for t in range(1,40001):\n",
    "    x,y = random_training_set_with_road_tar()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    L += sum(loss.data.numpy())\n",
    "    n += len(loss.data.numpy())\n",
    "    if t%1000 == 0:\n",
    "        pts['x'].append(t)\n",
    "        pts['y'].append(L/n)\n",
    "        print(t, L/n)\n",
    "        L = 0\n",
    "        n = 0\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = random_training_set_with_road_tar()\n",
    "print([inv_one_hot(r) for r in x.data.numpy()])\n",
    "np.concatenate([model(x)[0,:].data.numpy().reshape(24,1), \n",
    "                np.array(range(0,24)).reshape(24,1)],\n",
    "               axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_training_pair2(path):\n",
    "    return path[1:], path[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_next.view(-1,1).repeat(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0    \n",
    "print('x:', x[i],'nb:', graph.neighbors(x[i]))\n",
    "nb = graph.neighbors(x[i])\n",
    "W_list = [W[j].view(-1,1) for j in nb]\n",
    "Ws = torch.cat(W_list, dim = 1)\n",
    "W_next = model(W[x[0]])\n",
    "#print('W_next:', W_next)\n",
    "#print(W_next.size(), Ws.size())\n",
    "W_next_rep = W_next.view(-1,1).repeat(1,Ws.size()[1])\n",
    "#print(W_next_rep.size(), Ws.size())\n",
    "logit = torch.sum(W_next_rep*Ws, 0)\n",
    "prob = F.softmax(logit, dim = 0)\n",
    "print(prob)\n",
    "# loss = -torch.log(prob[np.where(nb == y[i])])\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(to_Variable([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_training_pair2(path):\n",
    "    return path[1:], path[:-1]\n",
    "\n",
    "def get_prob(x):\n",
    "    nb = graph.neighbors(x)\n",
    "    W_list = [W[j].view(-1,1) for j in nb]\n",
    "    Ws = torch.cat(W_list, dim = 1)\n",
    "    W_next = model(W[x])\n",
    "    #print('W_next:', W_next)\n",
    "    #print(W_next.size(), Ws.size())\n",
    "    W_next_rep = W_next.view(-1,1).repeat(1,Ws.size()[1])\n",
    "    #print(W_next_rep.size(), Ws.size())\n",
    "    logit = torch.sum(W_next_rep*Ws, 0)\n",
    "    prob = F.softmax(logit, dim = 0)\n",
    "    return prob\n",
    "\n",
    "def get_loss(x, y):\n",
    "    nb = graph.neighbors(x)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    W_list = [W[j].view(-1,1) for j in nb]\n",
    "    Ws = torch.cat(W_list, dim = 1)\n",
    "    W_next = model(W[x])\n",
    "    #print('W_next:', W_next)\n",
    "    #print(W_next.size(), Ws.size())\n",
    "    W_next_rep = W_next.view(-1,1).repeat(1,Ws.size()[1])\n",
    "    #print(W_next_rep.size(), Ws.size())\n",
    "    logit = torch.sum(W_next_rep*Ws, 0)\n",
    "    prob = F.softmax(logit, dim = 0)\n",
    "    loss = -torch.log(prob[np.where(nb == y)])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "\n",
    "data = random_walk_data(graph,10000)\n",
    "\n",
    "im_D = 5\n",
    "N = 24\n",
    "h_D = 100\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "W = Variable(torch.randn(N,im_D).type(dtype), requires_grad=True)\n",
    "model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(im_D, h_D),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(h_D, im_D),\n",
    "    )\n",
    "for k in range(len(data)):\n",
    "    path = data[k]\n",
    "    x,y = path_to_training_pair2(path)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + [W], lr=0.001)\n",
    "    for i in range(len(x)):\n",
    "        loss = get_loss(x[i], y[i])\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "    if k%50 == 0:\n",
    "        batch_loss = torch.sum(torch.cat(loss_list)) / len(loss_list)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list = []\n",
    "        print(batch_loss.data.numpy())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones(3) /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss():\n",
    "    s = 0\n",
    "    for i in range(24):\n",
    "        p_ = get_prob(i).data.numpy()\n",
    "        n = len(get_prob(i))\n",
    "        s += -np.sum(np.ones(n) /n * np.log(p_))\n",
    "    return s/24\n",
    "log_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esay Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = [1/3,1/3,1/3]\n",
    "y = [1/3,1/3,1/3]\n",
    "-np.sum(y*np.log(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "x.backward()\n",
    "optimizer.step()\n",
    "x.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
