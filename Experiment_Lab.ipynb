{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch import nn, autograd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment One\n",
    "$$x \\to Bernulli(0.5)$$ \n",
    "$$y|x = 0 \\to Bernulli(0.9)$$ \n",
    "$$y|x = 1 \\to Bernulli(0.3)$$\n",
    "\n",
    "we use a linear-> sigmoid regression to learn the conditional probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and model\n",
    "sig = nn.Sigmoid()\n",
    "W = Variable(torch.rand(1), requires_grad=True)\n",
    "b = Variable(torch.rand(1), requires_grad=True)\n",
    "def net(x):\n",
    "    y_ = W*x + b\n",
    "    y_ = sig(y_)\n",
    "    loss = -(y*torch.log(y_) + (1-y)*torch.log(1-y_)) \n",
    "    return y_, loss\n",
    "\n",
    "def gen_data():\n",
    "    x = Variable(torch.bernoulli(torch.ones(1)*0.5))\n",
    "    if x.data.numpy() == 0:\n",
    "        y = Variable(torch.bernoulli(torch.ones(1)*0.9))\n",
    "    else:\n",
    "        y = Variable(torch.bernoulli(torch.ones(1)*0.3))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "for t in range(1,2000):\n",
    "    x,y = gen_data()\n",
    "    y_, loss = net(x)\n",
    "    loss.backward()\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "    b.grad.data.zero_()\n",
    "    W.data -= learning_rate * W.grad.data\n",
    "    W.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True para is 0.3, the perdiction is [ 0.29190451]\n",
      "True para is 0.9, the perdiction is [ 0.91689456]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "x = Variable(torch.ones(1))\n",
    "y_,_ = net(x)\n",
    "print('True para is 0.3, the perdiction is {}'.format(y_.data.numpy()))\n",
    "\n",
    "x = Variable(torch.ones(1)*0)\n",
    "y_,_ = net(x)\n",
    "print('True para is 0.9, the perdiction is {}'.format(y_.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in, D_out = 1, 1\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_out),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    x,y = gen_data()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True para is 0.3, the perdiction is [ 0.34832135]\n",
      "True para is 0.9, the perdiction is [ 0.90798932]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "x = Variable(torch.ones(1))\n",
    "y_ = model(x)\n",
    "print('True para is 0.3, the perdiction is {}'.format(y_.data.numpy()))\n",
    "\n",
    "x = Variable(torch.ones(1)*0)\n",
    "y_ = model(x)\n",
    "print('True para is 0.9, the perdiction is {}'.format(y_.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, D_out =50, 1, 1\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, D_out),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    data = [gen_data() for _ in range(N)]\n",
    "    x = torch.cat([i for i,_ in data]).view(N, D_in)\n",
    "    y = torch.cat([j for _,j in data]).view(N, D_out)\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    #if t%100 == 0:\n",
    "    #    print(t, loss.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True para is 0.3, the perdiction is [ 0.2888166]\n",
      "True para is 0.9, the perdiction is [ 0.90217209]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "x = Variable(torch.ones(1))\n",
    "y_ = model(x)\n",
    "print('True para is 0.3, the perdiction is {}'.format(y_.data.numpy()))\n",
    "\n",
    "x = Variable(torch.ones(1)*0)\n",
    "y_ = model(x)\n",
    "print('True para is 0.9, the perdiction is {}'.format(y_.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning road transition probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.8338 -1.0994 -0.3175\n",
      "-0.7335  0.8429  1.9418\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Variable containing:\n",
      " 0.8550  0.0455  0.0995\n",
      " 0.0491  0.2377  0.7132\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    ">>> m = nn.Softmax()\n",
    ">>> input = autograd.Variable(torch.randn(2, 3))\n",
    ">>> print(input)\n",
    ">>> print(m(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
